{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class-conditioned generative models are usually pre-trained on a class-labeled dataset such as ImageNet-1k. Popular metrics for evaluating these models include Fréchet Inception Distance (FID), Kernel Inception Distance (KID), and Inception Score (IS). In this document, we focus on FID (Heusel et al.). We show how to compute it with the DiTPipeline, which uses the DiT model under the hood.\n",
    "\n",
    "FID aims to measure how similar are two datasets of images. As per this resource:\n",
    "\n",
    "Fréchet Inception Distance is a measure of similarity between two datasets of images. It was shown to correlate well with the human judgment of visual quality and is most often used to evaluate the quality of samples of Generative Adversarial Networks. FID is calculated by computing the Fréchet distance between two Gaussians fitted to feature representations of the Inception network.\n",
    "\n",
    "These two datasets are essentially the dataset of real images and the dataset of fake images (generated images in our case). FID is usually calculated with two large datasets. However, for this document, we will work with two mini datasets.\n",
    "\n",
    "Let’s first download a few images from the ImageNet-1k training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "def download(url, local_filepath):\n",
    "    r = requests.get(url)\n",
    "    with open(local_filepath, \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    return local_filepath\n",
    "\n",
    "\n",
    "dummy_dataset_url = \"https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/sample-imagenet-images.zip\"\n",
    "local_filepath = download(dummy_dataset_url, dummy_dataset_url.split(\"/\")[-1])\n",
    "\n",
    "with ZipFile(local_filepath, \"r\") as zipper:\n",
    "    zipper.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "dataset_path = \"sample-imagenet-images\"\n",
    "image_paths = sorted([os.path.join(dataset_path, x) for x in os.listdir(dataset_path)])\n",
    "\n",
    "real_images = [np.array(Image.open(path).convert(\"RGB\")) for path in image_paths]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the images are loaded, let’s apply some lightweight pre-processing on them to use them for FID calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = torch.tensor(image).unsqueeze(0)\n",
    "    image = image.permute(0, 3, 1, 2) / 255.0\n",
    "    return F.center_crop(image, (256, 256))\n",
    "\n",
    "\n",
    "real_images = torch.cat([preprocess_image(image) for image in real_images])\n",
    "print(real_images.shape)\n",
    "# torch.Size([10, 3, 256, 256])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the DiTPipeline to generate images conditioned on the above-mentioned classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiTPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "dit_pipeline = DiTPipeline.from_pretrained(\"facebook/DiT-XL-2-256\", torch_dtype=torch.float16)\n",
    "dit_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(dit_pipeline.scheduler.config)\n",
    "dit_pipeline = dit_pipeline.to(\"cuda\")\n",
    "\n",
    "words = [\n",
    "    \"cassette player\",\n",
    "    \"chainsaw\",\n",
    "    \"chainsaw\",\n",
    "    \"church\",\n",
    "    \"gas pump\",\n",
    "    \"gas pump\",\n",
    "    \"gas pump\",\n",
    "    \"parachute\",\n",
    "    \"parachute\",\n",
    "    \"tench\",\n",
    "]\n",
    "\n",
    "class_ids = dit_pipeline.get_label_ids(words)\n",
    "output = dit_pipeline(class_labels=class_ids, generator=generator, output_type=\"numpy\")\n",
    "\n",
    "fake_images = output.images\n",
    "fake_images = torch.tensor(fake_images)\n",
    "fake_images = fake_images.permute(0, 3, 1, 2)\n",
    "print(fake_images.shape)\n",
    "# torch.Size([10, 3, 256, 256])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the FID using torchmetrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "fid = FrechetInceptionDistance(normalize=True)\n",
    "fid.update(real_images, real=True)\n",
    "fid.update(fake_images, real=False)\n",
    "\n",
    "print(f\"FID: {float(fid.compute())}\")\n",
    "# FID: 177.7147216796875"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the FID, the better it is. Several things can influence FID here:\n",
    "\n",
    "Number of images (both real and fake)\n",
    "Randomness induced in the diffusion process\n",
    "Number of inference steps in the diffusion process\n",
    "The scheduler being used in the diffusion process\n",
    "For the last two points, it is, therefore, a good practice to run the evaluation across different seeds and inference steps, and then report an average result.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
